# LLM Settings
OLLAMA_HOST=http://localhost:11434

# Main conversational model (larger = better quality, slower)
# Options: qwen3:14b (default), qwen3:8b, qwen3:4b, llama3.1:8b, etc.
MAIN_MODEL=qwen3:14b

# Observer model for memory extraction (smaller = faster, less accurate)
# Options: qwen3:1.7b (default), qwen3:4b (better), qwen3:0.6b (faster but unreliable)
# Note: 0.6b has JSON formatting issues, not recommended
OBSERVER_MODEL=qwen3:1.7b

# Embedding model for vector search (don't change unless you know what you're doing)
# Options: nomic-embed-text (default), mxbai-embed-large, all-minilm
EMBEDDING_MODEL=nomic-embed-text

# Database Paths
LANCEDB_PATH=./data/lancedb
FALKORDB_HOST=localhost
FALKORDB_PORT=6379
FALKORDB_GRAPH_ID=lcr_memories
REDIS_HOST=localhost
REDIS_PORT=6380

# Memory Settings
MAX_CONTEXT_TOKENS=3000
SLIDING_WINDOW_TOKENS=2000
TEMPORAL_DECAY_DAYS=30
VECTOR_SEARCH_TOP_K=15
GRAPH_SEARCH_TOP_K=10
RERANK_TOP_K=5

# Chunking Settings
SIMILARITY_THRESHOLD=0.85
MIN_CHUNK_SENTENCES=3
MAX_CHUNK_TOKENS=512

# Voice Settings (not implemented yet)
WHISPER_MODEL=medium
PIPER_VOICE=en_US-lessac-medium
WAKE_WORD=hey assistant

# Logging
LOG_LEVEL=INFO
